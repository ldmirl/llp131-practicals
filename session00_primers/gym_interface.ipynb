{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if \"google.colab\" in sys.modules and not os.path.exists(\".setup_complete\"):\n",
    "    !wget -q https://raw.githubusercontent.com/ldmirl/llp131-practicals/master/setup_colab.sh -O- | bash\n",
    "    !touch .setup_complete\n",
    "    !wget -q https://raw.githubusercontent.com/ldmirl/llp131-practicals/master/requirements.txt\n",
    "    !pip3 install -r requirements.txt\n",
    "elif \"gym\" not in sys.modules:\n",
    "    # Requirements file is already available in the repo if downloaded according to the instructions\n",
    "    !pip3 install -r ../requirements.txt\n",
    "\n",
    "# This code creates a virtual display to draw game images.\n",
    "# It will have no effect if your machine has a monitor\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ[\"DISPLAY\"] = \":1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI GYM\n",
    "\n",
    "We are going to spend our time learning algorithms that solve decision processes. We are then in need of some interesting decision problems to test our algorithms. Implementing each of these problems ourselves would be tedious. That is where OpenAI GYM comes into play. It is a Python library that wraps many classical decision problems including robot control, video-games, and board-games.\n",
    "\n",
    "__Here's how it works:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: if you are running this on your local machine, you will see a window pop up with the same image below. ***Don't close it!*** Just `alt-tab` away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "env.reset()\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\")); env.close()\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GYM Interface\n",
    "\n",
    "The three main methods of an environment are:\n",
    "\n",
    "| Method | Description |\n",
    "|:--|:--|\n",
    "| `reset()` | Reset the environment to the initial state, return *first* observation |\n",
    "| `render()` | Show current environment state (a more colorful version atleast) |\n",
    "| `step(a)` | Perform action `a` and return `(next_observation, reward, is_done, info)` |\n",
    "\n",
    "\n",
    "The four values returned from an environment `step(a)` are:\n",
    "\n",
    "| Value | Description |\n",
    "|:--|:--|\n",
    "|`next_observation` | The next observation after performing action `a` |\n",
    "| `reward` | A number representing the reward for performing action `a` |\n",
    "| `is_done` | `True` if the MDP finished, `False` if still in progress |\n",
    "| `info` | Additionally auxiliary information about what just happened. For now, ignore it. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs0 = env.reset()\n",
    "print(\"Initial observation:\", obs0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: In `MountainCar`, an observation is 2 bounded values: `car_position`, `car_velocity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Taking action 1 (right)\")\n",
    "next_obs, reward, is_done, _ = env.step(2)\n",
    "\n",
    "print(\"Next observation:\", next_obs)\n",
    "print(\"Reward:\", reward)\n",
    "print(\"Game Over?\", is_done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tinker with it!\n",
    "\n",
    "Below is the code that drives the car to the right. However, if you simply use this default policy, the car will not reach the flag at the far right due to gravity.\n",
    "\n",
    "**Your task** is to fix it. Find a strategy that reaches the flag.\n",
    "\n",
    "**Note**: You are ***NOT*** required to build any sophisticated algorithms and you definitely don't need to know any reinforcement learning for this. Feel free to hard-code it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "# Create env manually to set time limit. Please do not change this.\n",
    "TIME_LIMIT = 250\n",
    "env = gym.wrappers.TimeLimit(\n",
    "    gym.envs.classic_control.MountainCarEnv(),\n",
    "    max_episode_steps=TIME_LIMIT + 1,\n",
    ")\n",
    "actions = {\"left\": 0, \"stop\": 1, \"right\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(obs , t):\n",
    "    # Write the code for your policy here. You can use the observation\n",
    "    # (a tuple of position and velocity), the current time step, or both,\n",
    "    # if you want.\n",
    "    position, velocity = obs\n",
    "\n",
    "    # This is an example policy. You can try running it, but it will not work.\n",
    "    # Your goal is to fix that. You don't need anything sophisticated here,\n",
    "    # and you can hard-code any policy that seems to work.\n",
    "    # Hint: Think how you make a swing go farther and faster\n",
    "    return actions[\"right\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "display.clear_output(wait=True)\n",
    "\n",
    "obs = env.reset()\n",
    "for t in range(TIME_LIMIT):\n",
    "    plt.gca().clear()\n",
    "\n",
    "    action = policy(obs, t) # call your policy\n",
    "    obs, reward, done, _ = env.step(action) # pass the action chosen by the policy to the environment\n",
    "\n",
    "    # We do not do anything with reward here because MountainCar is a very simple environment,\n",
    "    # and reward is a constant -1. Therefore, your goal is to end the episode as quickly as possible.\n",
    "\n",
    "    # Draw the game image on display\n",
    "    plt.imshow(env.render(\"rgb_array\"))\n",
    "\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    if done:\n",
    "        print(\"Well done!\")\n",
    "        break\n",
    "else:\n",
    "    print(\"Time limit exceeded. Try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert obs[0] > 0.47\n",
    "print(\"You solved it!\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
